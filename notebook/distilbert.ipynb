{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting beautifulsoup4 (from gdown)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\mndpp\\desktop\\github_projects\\01_movie_review_sentiment\\venv\\lib\\site-packages (from gdown) (3.13.3)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\mndpp\\desktop\\github_projects\\01_movie_review_sentiment\\venv\\lib\\site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mndpp\\desktop\\github_projects\\01_movie_review_sentiment\\venv\\lib\\site-packages (from gdown) (4.66.2)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->gdown)\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mndpp\\desktop\\github_projects\\01_movie_review_sentiment\\venv\\lib\\site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mndpp\\desktop\\github_projects\\01_movie_review_sentiment\\venv\\lib\\site-packages (from requests[socks]->gdown) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mndpp\\desktop\\github_projects\\01_movie_review_sentiment\\venv\\lib\\site-packages (from requests[socks]->gdown) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mndpp\\desktop\\github_projects\\01_movie_review_sentiment\\venv\\lib\\site-packages (from requests[socks]->gdown) (2024.2.2)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\mndpp\\desktop\\github_projects\\01_movie_review_sentiment\\venv\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "   ---------------------------------------- 0.0/147.9 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/147.9 kB ? eta -:--:--\n",
      "   ---------- ---------------------------- 41.0/147.9 kB 495.5 kB/s eta 0:00:01\n",
      "   ------------------------ -------------- 92.2/147.9 kB 751.6 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 122.9/147.9 kB 804.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- 147.9/147.9 kB 800.5 kB/s eta 0:00:00\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, PySocks, beautifulsoup4, gdown\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.12.3 gdown-5.1.0 soupsieve-2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ZDFmDZDOi_hrfXrKYHjt9RorHBEcT1mq\n",
      "To: c:\\Users\\mndpp\\Desktop\\Github_Projects\\01_movie_review_sentiment\\notebook\\imdb_data.csv\n",
      "\n",
      "  0%|          | 0.00/66.0M [00:00<?, ?B/s]\n",
      "  1%|          | 524k/66.0M [00:00<00:12, 5.07MB/s]\n",
      "  2%|▏         | 1.05M/66.0M [00:00<00:15, 4.28MB/s]\n",
      "  2%|▏         | 1.57M/66.0M [00:00<00:14, 4.47MB/s]\n",
      "  3%|▎         | 2.10M/66.0M [00:00<00:14, 4.53MB/s]\n",
      "  4%|▍         | 2.62M/66.0M [00:00<00:17, 3.57MB/s]\n",
      "  5%|▍         | 3.15M/66.0M [00:00<00:17, 3.64MB/s]\n",
      "  6%|▌         | 3.67M/66.0M [00:00<00:17, 3.66MB/s]\n",
      "  6%|▋         | 4.19M/66.0M [00:01<00:16, 3.66MB/s]\n",
      "  7%|▋         | 4.72M/66.0M [00:01<00:16, 3.69MB/s]\n",
      "  8%|▊         | 5.24M/66.0M [00:01<00:17, 3.57MB/s]\n",
      "  9%|▊         | 5.77M/66.0M [00:01<00:16, 3.70MB/s]\n",
      " 10%|▉         | 6.29M/66.0M [00:01<00:16, 3.69MB/s]\n",
      " 10%|█         | 6.82M/66.0M [00:01<00:15, 3.90MB/s]\n",
      " 11%|█         | 7.34M/66.0M [00:01<00:14, 4.09MB/s]\n",
      " 12%|█▏        | 7.86M/66.0M [00:01<00:13, 4.27MB/s]\n",
      " 13%|█▎        | 8.39M/66.0M [00:02<00:13, 4.42MB/s]\n",
      " 14%|█▎        | 8.91M/66.0M [00:02<00:12, 4.51MB/s]\n",
      " 14%|█▍        | 9.44M/66.0M [00:02<00:12, 4.52MB/s]\n",
      " 15%|█▌        | 9.96M/66.0M [00:02<00:12, 4.54MB/s]\n",
      " 16%|█▌        | 10.5M/66.0M [00:02<00:13, 4.16MB/s]\n",
      " 17%|█▋        | 11.0M/66.0M [00:02<00:13, 4.04MB/s]\n",
      " 17%|█▋        | 11.5M/66.0M [00:02<00:13, 4.13MB/s]\n",
      " 18%|█▊        | 12.1M/66.0M [00:02<00:12, 4.29MB/s]\n",
      " 19%|█▉        | 12.6M/66.0M [00:03<00:12, 4.40MB/s]\n",
      " 20%|█▉        | 13.1M/66.0M [00:03<00:11, 4.46MB/s]\n",
      " 21%|██        | 13.6M/66.0M [00:03<00:11, 4.44MB/s]\n",
      " 21%|██▏       | 14.2M/66.0M [00:03<00:11, 4.46MB/s]\n",
      " 22%|██▏       | 14.7M/66.0M [00:03<00:11, 4.45MB/s]\n",
      " 23%|██▎       | 15.2M/66.0M [00:03<00:11, 4.57MB/s]\n",
      " 24%|██▍       | 15.7M/66.0M [00:03<00:10, 4.62MB/s]\n",
      " 25%|██▍       | 16.3M/66.0M [00:03<00:11, 4.40MB/s]\n",
      " 25%|██▌       | 16.8M/66.0M [00:04<00:11, 4.44MB/s]\n",
      " 26%|██▌       | 17.3M/66.0M [00:04<00:10, 4.50MB/s]\n",
      " 27%|██▋       | 17.8M/66.0M [00:04<00:10, 4.57MB/s]\n",
      " 28%|██▊       | 18.4M/66.0M [00:04<00:10, 4.61MB/s]\n",
      " 29%|██▊       | 18.9M/66.0M [00:04<00:10, 4.54MB/s]\n",
      " 29%|██▉       | 19.4M/66.0M [00:04<00:14, 3.17MB/s]\n",
      " 30%|███       | 19.9M/66.0M [00:04<00:13, 3.37MB/s]\n",
      " 31%|███       | 20.4M/66.0M [00:05<00:12, 3.52MB/s]\n",
      " 32%|███▏      | 21.0M/66.0M [00:05<00:12, 3.62MB/s]\n",
      " 33%|███▎      | 21.5M/66.0M [00:05<00:11, 3.75MB/s]\n",
      " 33%|███▎      | 22.0M/66.0M [00:05<00:10, 4.01MB/s]\n",
      " 34%|███▍      | 22.5M/66.0M [00:05<00:10, 4.22MB/s]\n",
      " 35%|███▍      | 23.1M/66.0M [00:05<00:09, 4.37MB/s]\n",
      " 36%|███▌      | 23.6M/66.0M [00:05<00:09, 4.41MB/s]\n",
      " 37%|███▋      | 24.1M/66.0M [00:05<00:09, 4.46MB/s]\n",
      " 37%|███▋      | 24.6M/66.0M [00:05<00:09, 4.41MB/s]\n",
      " 38%|███▊      | 25.2M/66.0M [00:06<00:09, 4.51MB/s]\n",
      " 39%|███▉      | 25.7M/66.0M [00:06<00:09, 4.29MB/s]\n",
      " 40%|███▉      | 26.2M/66.0M [00:06<00:09, 4.37MB/s]\n",
      " 41%|████      | 26.7M/66.0M [00:06<00:11, 3.53MB/s]\n",
      " 41%|████▏     | 27.3M/66.0M [00:06<00:11, 3.42MB/s]\n",
      " 42%|████▏     | 27.8M/66.0M [00:06<00:11, 3.21MB/s]\n",
      " 43%|████▎     | 28.3M/66.0M [00:07<00:10, 3.53MB/s]\n",
      " 44%|████▎     | 28.8M/66.0M [00:07<00:09, 3.78MB/s]\n",
      " 44%|████▍     | 29.4M/66.0M [00:07<00:09, 3.96MB/s]\n",
      " 45%|████▌     | 29.9M/66.0M [00:07<00:08, 4.14MB/s]\n",
      " 46%|████▌     | 30.4M/66.0M [00:07<00:08, 4.21MB/s]\n",
      " 47%|████▋     | 30.9M/66.0M [00:07<00:08, 4.31MB/s]\n",
      " 48%|████▊     | 31.5M/66.0M [00:07<00:07, 4.48MB/s]\n",
      " 48%|████▊     | 32.0M/66.0M [00:07<00:07, 4.27MB/s]\n",
      " 49%|████▉     | 32.5M/66.0M [00:07<00:07, 4.35MB/s]\n",
      " 50%|█████     | 33.0M/66.0M [00:08<00:07, 4.44MB/s]\n",
      " 51%|█████     | 33.6M/66.0M [00:08<00:07, 4.45MB/s]\n",
      " 52%|█████▏    | 34.1M/66.0M [00:08<00:07, 4.51MB/s]\n",
      " 52%|█████▏    | 34.6M/66.0M [00:08<00:07, 4.24MB/s]\n",
      " 53%|█████▎    | 35.1M/66.0M [00:08<00:07, 4.35MB/s]\n",
      " 54%|█████▍    | 35.7M/66.0M [00:08<00:07, 4.15MB/s]\n",
      " 55%|█████▍    | 36.2M/66.0M [00:08<00:07, 4.14MB/s]\n",
      " 56%|█████▌    | 36.7M/66.0M [00:08<00:06, 4.20MB/s]\n",
      " 56%|█████▋    | 37.2M/66.0M [00:09<00:06, 4.23MB/s]\n",
      " 57%|█████▋    | 37.7M/66.0M [00:09<00:06, 4.37MB/s]\n",
      " 58%|█████▊    | 38.3M/66.0M [00:09<00:06, 4.43MB/s]\n",
      " 59%|█████▉    | 38.8M/66.0M [00:09<00:06, 4.18MB/s]\n",
      " 60%|█████▉    | 39.3M/66.0M [00:09<00:08, 3.24MB/s]\n",
      " 60%|██████    | 39.8M/66.0M [00:09<00:07, 3.36MB/s]\n",
      " 61%|██████    | 40.4M/66.0M [00:09<00:07, 3.63MB/s]\n",
      " 62%|██████▏   | 40.9M/66.0M [00:10<00:07, 3.26MB/s]\n",
      " 63%|██████▎   | 41.4M/66.0M [00:10<00:07, 3.16MB/s]\n",
      " 64%|██████▎   | 41.9M/66.0M [00:10<00:07, 3.06MB/s]\n",
      " 64%|██████▍   | 42.5M/66.0M [00:10<00:07, 3.27MB/s]\n",
      " 65%|██████▌   | 43.0M/66.0M [00:10<00:08, 2.80MB/s]\n",
      " 66%|██████▌   | 43.5M/66.0M [00:11<00:07, 2.98MB/s]\n",
      " 67%|██████▋   | 44.0M/66.0M [00:11<00:06, 3.15MB/s]\n",
      " 68%|██████▊   | 44.6M/66.0M [00:11<00:06, 3.19MB/s]\n",
      " 68%|██████▊   | 45.1M/66.0M [00:11<00:06, 3.30MB/s]\n",
      " 69%|██████▉   | 45.6M/66.0M [00:11<00:05, 3.43MB/s]\n",
      " 70%|██████▉   | 46.1M/66.0M [00:11<00:05, 3.53MB/s]\n",
      " 71%|███████   | 46.7M/66.0M [00:11<00:05, 3.68MB/s]\n",
      " 72%|███████▏  | 47.7M/66.0M [00:12<00:04, 4.34MB/s]\n",
      " 74%|███████▍  | 48.8M/66.0M [00:12<00:03, 4.77MB/s]\n",
      " 75%|███████▍  | 49.3M/66.0M [00:12<00:04, 4.12MB/s]\n",
      " 76%|███████▌  | 50.3M/66.0M [00:12<00:03, 4.50MB/s]\n",
      " 78%|███████▊  | 51.4M/66.0M [00:12<00:03, 4.82MB/s]\n",
      " 79%|███████▊  | 51.9M/66.0M [00:12<00:02, 4.88MB/s]\n",
      " 79%|███████▉  | 52.4M/66.0M [00:13<00:02, 4.93MB/s]\n",
      " 81%|████████  | 53.5M/66.0M [00:13<00:02, 5.15MB/s]\n",
      " 83%|████████▎ | 54.5M/66.0M [00:13<00:02, 5.24MB/s]\n",
      " 83%|████████▎ | 55.1M/66.0M [00:13<00:02, 5.23MB/s]\n",
      " 85%|████████▍ | 56.1M/66.0M [00:13<00:01, 5.26MB/s]\n",
      " 87%|████████▋ | 57.1M/66.0M [00:13<00:01, 5.46MB/s]\n",
      " 88%|████████▊ | 58.2M/66.0M [00:14<00:01, 5.46MB/s]\n",
      " 90%|████████▉ | 59.2M/66.0M [00:14<00:01, 5.31MB/s]\n",
      " 91%|█████████▏| 60.3M/66.0M [00:14<00:01, 5.35MB/s]\n",
      " 93%|█████████▎| 61.3M/66.0M [00:14<00:00, 5.36MB/s]\n",
      " 95%|█████████▍| 62.4M/66.0M [00:14<00:00, 5.29MB/s]\n",
      " 96%|█████████▌| 63.4M/66.0M [00:15<00:00, 5.32MB/s]\n",
      " 98%|█████████▊| 64.5M/66.0M [00:15<00:00, 5.47MB/s]\n",
      " 99%|█████████▉| 65.5M/66.0M [00:15<00:00, 5.25MB/s]\n",
      "100%|██████████| 66.0M/66.0M [00:15<00:00, 4.22MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown 1ZDFmDZDOi_hrfXrKYHjt9RorHBEcT1mq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Imagine The Big Chill with a cast of twenty-so...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'd have to say that I've seen worse Sci Fi Ch...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Director Fabio Barreto got a strange Academy N...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pretty bad PRC cheapie which I rarely bother t...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a very intriguing short movie by David...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating  Sentiment\n",
       "0  Imagine The Big Chill with a cast of twenty-so...       2          0\n",
       "1  I'd have to say that I've seen worse Sci Fi Ch...       3          0\n",
       "2  Director Fabio Barreto got a strange Academy N...       1          0\n",
       "3  Pretty bad PRC cheapie which I rarely bother t...       4          0\n",
       "4  This is a very intriguing short movie by David...       8          1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(Path(os.path.join(os.getcwd(),'imdb_data.csv')))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    # Replacing n't with not since it could be really important in sentiment analysis\n",
    "    text = re.sub(\"n't\", ' not ', text)\n",
    "    # Removing URLs\n",
    "    text = re.sub('(http).*\\/', ' ', text)\n",
    "    # Removing HTML tags\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    # Extracting emoticons\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|P|D|]|})', text)\n",
    "    # Removing punctuations\n",
    "    text = re.sub('[\\W]+', ' ', text.lower())\n",
    "    # Adding emoticons at end and converting :-) to :)\n",
    "    text = text + ' ' + ' '.join(emoticons).replace('-', '')\n",
    "    return text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imagine the big chill with a cast of twenty so...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i d have to say that i ve seen worse sci fi ch...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>director fabio barreto got a strange academy n...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pretty bad prc cheapie which i rarely bother t...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is a very intriguing short movie by david...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating  Sentiment\n",
       "0  imagine the big chill with a cast of twenty so...       2          0\n",
       "1  i d have to say that i ve seen worse sci fi ch...       3          0\n",
       "2  director fabio barreto got a strange academy n...       1          0\n",
       "3  pretty bad prc cheapie which i rarely bother t...       4          0\n",
       "4  this is a very intriguing short movie by david...       8          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Review'] = df['Review'].apply(text_preprocessing)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into Train, Test, Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Review'].values \n",
    "y = df['Sentiment'].values \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained( 'distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(list(X_train), truncation=True, padding=True)\n",
    "valid_tokenized = tokenizer(list(X_valid), truncation=True, padding=True)\n",
    "test_tokenized = tokenizer(list(X_test), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized, labels):\n",
    "        super().__init__()\n",
    "        self.tokenized = tokenized\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = {key: torch.tensor(value[index]) for key, value in train_tokenized.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[index])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(train_tokenized, y_train)\n",
    "train_loader = DataLoader(train_data, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "valid_data = CustomDataset(valid_tokenized, y_valid)\n",
    "valid_loader = DataLoader(valid_data, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_data = CustomDataset(test_tokenized, y_test)\n",
    "test_loader = DataLoader(test_data, BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader):\n",
    "    accuracy = 0.0\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for _, batch in enumerate(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask = attention_mask, labels = labels)\n",
    "            loss = outputs['loss']\n",
    "            total_loss += loss.item()*len(labels)\n",
    "            logits = outputs['logits']\n",
    "            y_preds = torch.argmax(logits, 1)\n",
    "            correct_counts = (y_preds == labels).float().sum().item()\n",
    "            accuracy += correct_counts \n",
    "    accuracy = accuracy/len(data_loader.dataset)\n",
    "    total_loss = total_loss/len(data_loader.dataset)  \n",
    "    return accuracy, total_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, model_name, optimizer, train_data_loader, valid_data_loader, num_epochs = 10):\n",
    "    history = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask = attention_mask, labels = labels)\n",
    "            \n",
    "            loss = outputs['loss']\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 200 == 0 :\n",
    "                print(f'Epoch No. {epoch}/{num_epochs} | Batch No. {batch_idx}/{len(train_data_loader)} | Loss = {loss:.5f}')\n",
    "        training_accuracy, training_loss = get_accuracy(model, train_data_loader)*100\n",
    "        valid_accuracy, valid_loss = get_accuracy(model, valid_data_loader)*100\n",
    "        print(f'Training Accuracy = {training_accuracy:.2f}%, Loss = {training_loss:.4f}')\n",
    "        print(f'Valid Accuracy = {valid_accuracy:.2f}%, Loss = {valid_loss:.4f}')\n",
    "        history.append([training_accuracy, training_loss, valid_accuracy, valid_loss])\n",
    "    return model, history    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_training(trained_model, test_loader, history, model_name):\n",
    "    test_acc, test_loss = get_accuracy(trained_model, test_loader)\n",
    "    history_np = np.array(history)\n",
    "    fig, axes = plt.subplots(1, 2, figsize = (12, 4))\n",
    "    epochs_list = np.arange(1, len(history_np)+1)\n",
    "    axes[0].plot(epochs_list, history_np[:, 0], label = \"Training Accuracy\", marker = '.')\n",
    "    axes[0].plot(epochs_list, history_np[:, 2], label = \"Validation Accuracy\", marker = '.')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(epochs_list, history_np[:, 1], label = \"Training Loss\", marker = '.')\n",
    "    axes[1].plot(epochs_list, history_np[:, 3], label = \"Validation Loss\", marker = '.')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "\n",
    "    fig.suptitle(f'Training for {model_name}')\n",
    "    plt.show()\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using DistilBert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5195aa8e7ef4821a5dfd26c1bce4e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mndpp\\Desktop\\Github_Projects\\01_movie_review_sentiment\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mndpp\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "trained_model, history = training(model, 'DistilBERT', optimizer, train_loader, valid_loader, num_epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = post_training(trained_model, test_loader, history, 'DistilBERT')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
